pipeline {
  agent any
  environment {
    SPARK_MASTER_URL = 'spark://spark-master:7077'
    HDFS_URI         = 'hdfs://namenode:9000'
    TRAIN_SCRIPT     = '/scripts/train_model.py'
    MLFLOW_URL       = 'http://mlflow:5000'
    TRAINING_LOG     = '/tmp/arlequin-logs/training_log.csv'
    DRIFT_LOG        = '/tmp/arlequin-logs/drift_log.csv'
    MANN_LOG         = '/tmp/arlequin-logs/mannwhitney_results.csv'
    EVAL_SCENARIO    = ''
  }
  stages {
    stage('Seleccionar escenario') {
      steps {
        script {
          boolean remoteTriggered = false
          try {
            def remoteCauses = currentBuild.getBuildCauses('hudson.model.Cause$RemoteCause')
            remoteTriggered = remoteCauses && !remoteCauses.isEmpty()
          } catch (ignored) {
            remoteTriggered = false
          }
          if (!remoteTriggered) {
            def causeEnv = (env.BUILD_CAUSE ?: '')
            remoteTriggered = causeEnv.tokenize(',').any { it.trim().equalsIgnoreCase('REMOTECAUSE') }
          }

          String scenarioValue
          if (remoteTriggered) {
            scenarioValue = 'drift_detectado'
            echo "Build disparado remotamente (probable drift_watch). Se usará escenario '${scenarioValue}'."
          } else {
            def response = input(
              message: 'Selecciona el escenario a evaluar (pre/post)',
              ok: 'Continuar',
              parameters: [
                choice(
                  name: 'SCENARIO',
                  choices: ['pre', 'post'].join('\n'),
                  description: 'Etiqueta que se guardará en training_log.csv para distinguir corridas.'
                )
              ]
            )
            scenarioValue = (response instanceof Map) ? response['SCENARIO'] : response?.toString()
          }
          scenarioValue = scenarioValue?.trim()
          if (!scenarioValue) {
            scenarioValue = 'manual'
          }
          env.EVAL_SCENARIO = scenarioValue
          writeFile file: '.scenario', text: scenarioValue
          echo "Escenario seleccionado: ${scenarioValue}"
        }
      }
    }
    stage('Resolve containers') {
      steps {
        sh '''
          set -eu
          CID_PY=$(docker ps -qf "label=com.docker.compose.service=pyspark-client" || true)
          if [ -z "$CID_PY" ]; then CID_PY=$(docker ps -qf "name=pyspark-client"); fi
          [ -n "$CID_PY" ] || { echo "[ERROR] pyspark-client no encontrado"; docker ps; exit 1; }
          echo "$CID_PY" > .cid_py
        '''
      }
    }
    stage('Prep evaluator deps') {
      steps {
        sh '''
          set -eu
          CID_PY=$(cat .cid_py)
          docker exec -i "$CID_PY" sh -lc 'python3 - <<PY
try:
 import scipy, pandas, numpy
 print("eval deps OK")
except Exception:
 raise SystemExit(1)
PY' || docker exec -i "$CID_PY" sh -lc 'python3 -m pip install --no-cache-dir -r /scripts/requirements-drift.txt'
        '''
      }
    }
    stage('Evaluate latest partitions') {
      steps {
        sh '''
          set -eu
          CID_PY=$(cat .cid_py)
          SCENARIO="${EVAL_SCENARIO:-manual}"
          METRICS_DIR="$WORKSPACE/metrics"
          mkdir -p "$METRICS_DIR"

          RUNS_CSV="$METRICS_DIR/jenkins_runs.csv"
          if [ ! -f "$RUNS_CSV" ]; then
            echo "timestamp,build,stage,cpu_perc,mem_perc,mem_used_mb,mem_limit_mb,duration_s" > "$RUNS_CSV"
          fi

          start_ts=$(date +%s)
          stats=$(docker stats --no-stream --format '{{.CPUPerc}},{{.MemUsage}},{{.MemPerc}}' "$CID_PY" || echo ",,")
          cpu=$(echo "$stats" | cut -d, -f1 | tr -d '%')
          mem_perc=$(echo "$stats" | cut -d, -f3 | tr -d '%')
          mem_pair=$(echo "$stats" | cut -d, -f2 | tr -d ' ')
          used=$(echo "$mem_pair" | cut -d/ -f1 | sed 's/[^0-9.]*//g')
          limit=$(echo "$mem_pair" | cut -d/ -f2 | sed 's/[^0-9.]*//g')
          scenario_tag=$(printf '%s' "$SCENARIO" | tr '[:upper:]' '[:lower:]' | tr ' ' '_' | tr -cd 'a-z0-9._-')
          [ -n "$scenario_tag" ] || scenario_tag="manual"
          echo "$(date -Iseconds),$BUILD_NUMBER,eval-${scenario_tag}-pre,$cpu,$mem_perc,$used,$limit,0" >> "$RUNS_CSV"

          docker exec -i \
            -e MLFLOW_TRACKING_URI=${MLFLOW_URL} \
            -e EXPERIMENT_LOG_FILE=${TRAINING_LOG} \
            -e EVAL_SCENARIO="$SCENARIO" \
            "$CID_PY" sh -lc 'set -eu
              mkdir -p "$(dirname "$EXPERIMENT_LOG_FILE")"
              spark-submit --master '"${SPARK_MASTER_URL}"' --conf spark.hadoop.fs.defaultFS='"${HDFS_URI}"' '"${TRAIN_SCRIPT}"'
            '

          end_ts=$(date +%s)
          dur=$((end_ts - start_ts))
          stats=$(docker stats --no-stream --format '{{.CPUPerc}},{{.MemUsage}},{{.MemPerc}}' "$CID_PY" || echo ",,")
          cpu=$(echo "$stats" | cut -d, -f1 | tr -d '%')
          mem_perc=$(echo "$stats" | cut -d, -f3 | tr -d '%')
          mem_pair=$(echo "$stats" | cut -d, -f2 | tr -d ' ')
          used=$(echo "$mem_pair" | cut -d/ -f1 | sed 's/[^0-9.]*//g')
          limit=$(echo "$mem_pair" | cut -d/ -f2 | sed 's/[^0-9.]*//g')
          echo "$(date -Iseconds),$BUILD_NUMBER,eval-${scenario_tag}-post,$cpu,$mem_perc,$used,$limit,$dur" >> "$RUNS_CSV"

          docker cp "$CID_PY":"${TRAINING_LOG}" "$METRICS_DIR/training_log.csv" 2>/dev/null || true
          docker cp "$CID_PY":"${DRIFT_LOG}" "$METRICS_DIR/drift_log.csv" 2>/dev/null || true
        '''
      }
    }
    stage('Generar resumen de métricas') {
      steps {
        sh '''
          set -eu
          CID_PY=$(cat .cid_py)
          METRICS_DIR="$WORKSPACE/metrics"
          mkdir -p "$METRICS_DIR"
          docker exec -i "$CID_PY" sh -lc "python3 /scripts/eval_stats.py --input ${TRAINING_LOG} --drift-input ${DRIFT_LOG} --out ${MANN_LOG} --fdr || true"
          docker cp "$CID_PY":"${MANN_LOG}" "$METRICS_DIR/" 2>/dev/null || true
          docker cp "$CID_PY":"${TRAINING_LOG}" "$METRICS_DIR/training_log.csv" 2>/dev/null || true
          docker cp "$CID_PY":"${DRIFT_LOG}" "$METRICS_DIR/drift_log.csv" 2>/dev/null || true
        '''
      }
    }
  }
  post {
    always {
      echo 'Evaluation-only pipeline finished.'
    }
  }
}
