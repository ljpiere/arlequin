# version: '3.8'

x-airflow-common:
  &airflow-common
  image: apache/airflow:2.8.1-python3.9
  environment:
    AIRFLOW_HOME: /opt/airflow
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'

    # NUEVO nombre de clave (sección [database])
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow

    # Usa las claves correctas para Celery
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow

    DOCKER_HOST: unix:///var/run/docker.sock
    DOCKER_API_VERSION: "1.45"


  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - ./hadoop-base/hadoop-conf:/opt/hadoop/etc/hadoop

services:
  #-----------------------------------------------------------------
  # TRACKING PARA ML
  #-----------------------------------------------------------------
  # MLflow Tracking Server (simple, filesystem backend)
  mlflow:
    image: python:3.11-slim
    container_name: mlflow
    environment:
      MLFLOW_GUNICORN_CMD_ARGS: --bind 0.0.0.0:5000    # <- fuerza gunicorn a 0.0.0.0
    command: ["/bin/sh","-lc",
      "set -e; \
       pip install --no-cache-dir mlflow==2.14.* && \
       exec mlflow server \
         --backend-store-uri sqlite:////mlflow/mlflow.db \
         --default-artifact-root file:/mlflow/artifacts \
         --host 0.0.0.0 --port 5000"
    ]
    volumes:
      - ./mlflow:/mlflow
    ports:
      - "5000:5000"
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:5000/ || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 20
    networks: [default]
  #-----------------------------------------------------------------
  # Jenkins LTS (ejecutará spark-submit dentro de pyspark-client)
  #-----------------------------------------------------------------
  jenkins:
    image: jenkins/jenkins:lts
    container_name: jenkins
    user: root
    ports:
      - "8085:8080"               # UI: http://localhost:8085
      - "50000:50000"
    volumes:
      - ./jenkins:/var/jenkins_home
      - /var/run/docker.sock:/var/run/docker.sock
      - ./scripts:/scripts
      - ./:/workspace  
    environment:
      - JAVA_OPTS=-Djenkins.install.runSetupWizard=false
    networks: [default]
  #-----------------------------------------------------------------
  # Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    networks: [default]
  #-----------------------------------------------------------------
  # Grafana
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - ./grafana:/var/lib/grafana
    depends_on: [prometheus]
    networks: [default]
  #-----------------------------------------------------------------
  # Servicio “drift-watch” (exporta métricas /metrics y dispara Jenkins)
  drift-watch:
    image: python:3.11-slim
    container_name: drift-watch
    working_dir: /app
    environment:
      SPARK_MASTER_URL: "spark://spark-master:7077"
      HDFS_URI: "hdfs://namenode:9000"
      DATA_PATH: "hdfs:///datalake/raw/bank_transactions"
      MLFLOW_TRACKING_URI: "http://mlflow:5000"
      JENKINS_URL: "http://jenkins:8080"
      JENKINS_JOB: "retrain-model"
      JENKINS_TOKEN: "mytoken123"
      DRIFT_ALPHA: "0.01"
      LOOKBACK_HOURS: "1"
      REF_HOURS: "6"
      EXPORTER_PORT: "8010"
      JAVA_HOME: "/usr/lib/jvm/java-21-openjdk-amd64"
      PYSPARK_PYTHON: "/usr/local/bin/python"   # usa la ruta real del binario
      PYTHONUNBUFFERED: "1"
    volumes:
      - ./scripts:/app
    command: >
      sh -lc '
        set -euo pipefail
        export DEBIAN_FRONTEND=noninteractive

        # PATH “seguro” + Java
        export PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:$JAVA_HOME/bin"

        apt-get update &&
        apt-get install -y --no-install-recommends openjdk-21-jre-headless curl procps ca-certificates &&
        rm -rf /var/lib/apt/lists/*

        # Asegura que exista el alias python3 si alguna lib lo necesita
        [ -x /usr/local/bin/python3 ] || ln -s /usr/local/bin/python /usr/local/bin/python3

        # Instala deps si existe el requirements
        [ -f /app/requirements-drift.txt ] && /usr/local/bin/python -m pip install --no-cache-dir -r /app/requirements-drift.txt

        # Checks
        java -version
        ps --version || true
        /usr/local/bin/python -V
        which /usr/local/bin/python || true
        which python3 || true
        echo "PATH=$PATH"

        exec /usr/local/bin/python /app/drift_watch.py
      '
    ports:
      - "8010:8010"
    depends_on:
      - spark-master
      - mlflow
      - jenkins
    # restart: unless-stopped  # déjalo comentado mientras depuras
    networks: [default]





  #-----------------------------------------------------------------
  # TRACKING BIG DATA ARCHITECTURA
  #-----------------------------------------------------------------
  # Contenedor temporal: Copia de scripts
  scripts-init:
    image: alpine
    command: ["/bin/sh", "-c", "mkdir -p /dst && cp -r /src/* /dst/ || true && ls -l /dst"]
    volumes:
      - ./scripts:/src:ro
      - scripts_data:/dst
  #-----------------------------------------------------------------
  # NAMENODE: Configuracion base Hadoop
  namenode:
    build:
      context: ./hadoop-base
      dockerfile: Dockerfile
    hostname: namenode
    container_name: namenode
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - namenode_data:/tmp/hadoop-namenode
      - ./hadoop-base/hadoop-conf:/opt/hadoop/etc/hadoop
      - ./scripts:/scripts
    environment:
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    command: ["/bin/bash", "-c", "/scripts/start-namenode.sh && sleep infinity"]
    healthcheck:
      test: ["CMD", "hdfs", "dfsadmin", "-report"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s
    networks: [default]

  #-----------------------------------------------------------------
  # DATANODE: Inicio del datanode
  datanode:
    build:
      context: ./hadoop-base
      dockerfile: Dockerfile
    hostname: datanode
    container_name: datanode
    depends_on:
      namenode:
        condition: service_healthy
    volumes:
      - datanode1_data:/tmp/hadoop-datanode
      - ./hadoop-base/hadoop-conf:/opt/hadoop/etc/hadoop
    environment:
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    command: ["/bin/bash", "-c", "/opt/hadoop/bin/hdfs --daemon start datanode && sleep infinity"]
    networks: [default]
  #-----------------------------------------------------------------
  # Nodo principal de Spark
  spark-master:
    build:
      context: ./spark-base
      dockerfile: Dockerfile
    hostname: spark-master
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./spark-base/spark-conf:/opt/spark/conf
      - ./hadoop-base/hadoop-conf:/opt/hadoop/etc/hadoop
      - ./scripts:/scripts
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    command: ["/bin/bash", "-c", "/scripts/start-spark-master.sh && sleep infinity"]
    depends_on:
      namenode:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget -q http://spark-master:8080/ -O /dev/null || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 10s
    networks: [default]
  #-----------------------------------------------------------------
  # Worker 1 para Spark
  spark-worker:
    build:
      context: ./spark-base
      dockerfile: Dockerfile
    hostname: spark-worker
    container_name: spark-worker
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ./spark-base/spark-conf:/opt/spark/conf
      - ./hadoop-base/hadoop-conf:/opt/hadoop/etc/hadoop
      - ./scripts:/scripts 
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1g
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    command: ["/bin/bash", "-c", "/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 && sleep infinity"]
    networks: [default]
  #-----------------------------------------------------------------
  # Cliente de PySpark
  pyspark-client:
    image: arlequin-pyspark-client
    build:
      context: ./spark-base
      dockerfile: Dockerfile
    hostname: pyspark-client
    container_name: pyspark-client
    volumes:
      - ./spark-base/spark-conf:/opt/spark/conf
      - ./hadoop-base/hadoop-conf:/opt/hadoop/etc/hadoop
      - ./scripts:/scripts
    command: >
      bash -lc "
        apt-get update &&
        apt-get install -y --no-install-recommends procps curl &&
        rm -rf /var/lib/apt/lists/* &&
        python3 -m pip install --no-cache-dir --upgrade pip setuptools wheel &&
        python3 -m pip install --no-cache-dir -r /scripts/requirements.txt &&
        sleep infinity
      "
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - PYSPARK_PYTHON=python3
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    depends_on:
      spark-master:
        condition: service_healthy
      namenode:
        condition: service_healthy
    tty: true
    stdin_open: true
    networks: [default]
  #-----------------------------------------------------------------
  # Cliente Postgres: Para metadatos de Airflow
  postgres:
    image: postgres:13
    hostname: postgres
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 10s
    networks: [default]
  #-----------------------------------------------------------------
  # Redis: Config de Airflow
  redis:
    image: redis:latest
    hostname: redis
    container_name: redis
    ports:
      - "6379:6379"
    networks: [default]
  #-----------------------------------------------------------------
  # Init Airflow: Disparador
  airflow-init:
    <<: *airflow-common
    container_name: airflow_init
    user: "root"
    volumes:
      - ./scripts:/scripts
    command: bash -c "sleep 10 && airflow db migrate && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com"
    depends_on:
      postgres:
        condition: service_healthy
    networks: [default]
  #-----------------------------------------------------------------
  # UI Airflow
  airflow-webserver:
    <<: *airflow-common
    container_name: airflow_webserver
    command: airflow webserver
    ports:
      - "8082:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    networks: [default]
  #-----------------------------------------------------------------
  # Airflow scheduler
  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow_scheduler
    command: airflow scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    networks: [default]
  #-----------------------------------------------------------------
  # Worker de airflow
  airflow-worker:
    <<: *airflow-common
    container_name: airflow_worker
    command: airflow celery worker
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./hadoop-base/hadoop-conf:/opt/hadoop/etc/hadoop
      - scripts_data:/opt/airflow/scripts
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      airflow-scheduler:
        condition: service_started
      redis:
        condition: service_started
      postgres:
        condition: service_healthy
    networks: [default]
#-----------------------------------------------------------------
# Volumenes para archivos
volumes:
  namenode_data:
  datanode1_data:
  datanode2_data:
  pg_data:
  scripts_data:
    name: scripts_data
  spark_data:

#-----------------------------------------------------------------
# Conexión global
networks:
  default:
    name: arlequin_default
    driver: bridge