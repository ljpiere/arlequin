# version: '3.8'

x-airflow-common: &airflow-common
  image: apache/airflow:2.8.1-python3.9
  environment:
    AIRFLOW_HOME: /opt/airflow
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    DOCKER_HOST: unix:///var/run/docker.sock
    DOCKER_API_VERSION: "1.45"
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - ./hadoop-base/hadoop-conf:/opt/hadoop/etc/hadoop

services:
  #-----------------------------------------------------------------
  # TRACKING PARA ML
  #-----------------------------------------------------------------
  mlflow:
    image: python:3.11-slim
    container_name: mlflow
    environment:
      MLFLOW_GUNICORN_CMD_ARGS: --bind 0.0.0.0:5000
    command: ["/bin/sh","-lc",
      "set -e; \
       pip install --no-cache-dir mlflow==2.14.* && \
       exec mlflow server \
         --backend-store-uri sqlite:////mlflow/mlflow.db \
         --default-artifact-root file:/mlflow/artifacts \
         --host 0.0.0.0 --port 5000"
    ]
    volumes:
      - ./mlflow:/mlflow
    ports:
      - "5000:5000"
    healthcheck:
      test:
        ["CMD-SHELL", "python - <<'PY'\nimport urllib.request,sys\ntry:\n  urllib.request.urlopen('http://127.0.0.1:5000', timeout=3)\n  sys.exit(0)\nexcept Exception:\n  sys.exit(1)\nPY"]
      interval: 10s
      timeout: 5s
      retries: 20
    networks: [default]

  #-----------------------------------------------------------------
  # Jenkins LTS (se mantiene build local)
  #-----------------------------------------------------------------
  jenkins:
    build: ./jenkins
    container_name: jenkins
    user: root
    ports:
      - "8085:8080"
      - "50000:50000"
    volumes:
      - ./jenkins:/var/jenkins_home
      - /var/run/docker.sock:/var/run/docker.sock
      - ./scripts:/scripts
      - ./:/workspace
    environment:
      JENKINS_JAVA_OPTS: "-Xms256m -Xmx384m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -Djenkins.model.Jenkins.numExecutors=1"
    deploy:
      resources:
        limits:
          memory: 1g
        reservations:
          memory: 512m
    restart: unless-stopped
    networks: [default]

  #-----------------------------------------------------------------
  # Prometheus
  #-----------------------------------------------------------------
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    networks: [default]

  #-----------------------------------------------------------------
  # Grafana
  #-----------------------------------------------------------------
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - ./grafana:/var/lib/grafana
    depends_on: [prometheus]
    networks: [default]

  #-----------------------------------------------------------------
  # Servicio “drift-watch” (sin cambios funcionales)
  #-----------------------------------------------------------------
  drift-watch:
    image: python:3.11-slim
    container_name: drift-watch
    working_dir: /app
    env_file:
      - .env
    environment:
      #SPARK_MASTER_URL: "spark://spark-master:7077"
      SPARK_MASTER_URL: "local[1]"
      HDFS_URI: "hdfs://namenode:9000"
      DATA_PATH: "hdfs://namenode:9000/datalake/raw/bank_transactions"
      DRIFT_ALPHA: "${DRIFT_ALPHA:-0.01}"
      EXPORTER_PORT: "8010"
      JAVA_HOME: "/usr/lib/jvm/java-21-openjdk-amd64"
      PYSPARK_PYTHON: "/usr/local/bin/python"
      PYTHONUNBUFFERED: "1"
      PSI_ALERT: "-1"
      DRIFT_COOLDOWN_SECONDS: "1800"
      LOOP_SECONDS: "60"
      TRIGGER_EDGE_ONLY: "1"
    volumes:
      - ./scripts:/app
    command: >
      sh -lc '
        set -euo pipefail
        export DEBIAN_FRONTEND=noninteractive
        export PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:$JAVA_HOME/bin"
        apt-get update &&
        apt-get install -y --no-install-recommends openjdk-21-jre-headless curl procps ca-certificates &&
        rm -rf /var/lib/apt/lists/* &&
        [ -x /usr/local/bin/python3 ] || ln -s /usr/local/bin/python /usr/local/bin/python3 &&
        [ -f /app/requirements-drift.txt ] && /usr/local/bin/python -m pip install --no-cache-dir -r /app/requirements-drift.txt || true &&
        java -version && /usr/local/bin/python -V || true &&
        exec /usr/local/bin/python /app/drift_watch.py
      '
    ports:
      - "8010:8010"
    depends_on:
      - spark-master
      - mlflow
      - jenkins
      - namenode
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks: [default]

  #-----------------------------------------------------------------
  # TRACKING BIG DATA ARCHITECTURA
  #-----------------------------------------------------------------
  scripts-init:
    image: alpine
    command: ["/bin/sh", "-c", "mkdir -p /dst && cp -r /src/* /dst/ || true && ls -l /dst"]
    volumes:
      - ./scripts:/src:ro
      - scripts_data:/dst

  #=====================  IMÁGENES DESDE DOCKER HUB  =====================#
  # HDFS
  namenode:
    image: jeanpierec/arlequin-namenode:latest
    hostname: namenode
    container_name: namenode
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - namenode_data:/tmp/hadoop-namenode
      - ./hadoop-base/hadoop-conf:/opt/hadoop/etc/hadoop
      - ./scripts:/scripts
    environment:
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    command: ["/bin/bash", "-c", "/scripts/start-namenode.sh && sleep infinity"]
    healthcheck:
      test: ["CMD", "hdfs", "dfsadmin", "-report"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s
    networks: [default]

  datanode:
    image: jeanpierec/arlequin-datanode:latest
    hostname: datanode
    container_name: datanode
    depends_on:
      namenode:
        condition: service_healthy
    volumes:
      - datanode1_data:/tmp/hadoop-datanode
      - ./hadoop-base/hadoop-conf:/opt/hadoop/etc/hadoop
    environment:
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    command: ["/bin/bash", "-c", "/opt/hadoop/bin/hdfs --daemon start datanode && sleep infinity"]
    networks: [default]

  # Spark master/worker con imágenes publicadas
  spark-master:
    image: jeanpierec/arlequin-spark-master:latest
    hostname: spark-master
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./spark-base/spark-conf:/opt/spark/conf
      - ./hadoop-base/hadoop-conf:/opt/hadoop/etc/hadoop
      - ./scripts:/scripts
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    command: ["/bin/bash", "-c", "/scripts/start-spark-master.sh && sleep infinity"]
    depends_on:
      namenode:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget -q http://spark-master:8080/ -O /dev/null || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 10s
    networks: [default]

  spark-worker:
    image: jeanpierec/arlequin-spark-worker:latest
    hostname: spark-worker
    container_name: spark-worker
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ./spark-base/spark-conf:/opt/spark/conf
      - ./hadoop-base/hadoop-conf:/opt/hadoop/etc/hadoop
      - ./scripts:/scripts
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1g
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    command: ["/bin/bash", "-c", "/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 && sleep infinity"]
    networks: [default]

  pyspark-client:
    image: jeanpierec/arlequin-pyspark-client:latest
    hostname: pyspark-client
    container_name: pyspark-client
    volumes:
      - ./spark-base/spark-conf:/opt/spark/conf
      - ./hadoop-base/hadoop-conf:/opt/hadoop/etc/hadoop
      - ./scripts:/scripts
    command: >
      bash -lc "
        apt-get update &&
        apt-get install -y --no-install-recommends procps curl &&
        rm -rf /var/lib/apt/lists/* &&
        python3 -m pip install --no-cache-dir --upgrade pip setuptools wheel &&
        python3 -m pip install --no-cache-dir -r /scripts/requirements.txt &&
        sleep infinity
      "
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - PYSPARK_PYTHON=python3
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    depends_on:
      spark-master:
        condition: service_healthy
      namenode:
        condition: service_healthy
    tty: true
    stdin_open: true
    networks: [default]

  #=====================  FIN IMÁGENES DESDE DOCKER HUB  =====================#

  postgres:
    image: postgres:13
    hostname: postgres
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 10s
    networks: [default]

  redis:
    image: redis:latest
    hostname: redis
    container_name: redis
    ports:
      - "6379:6379"
    networks: [default]

  airflow-init:
    <<: *airflow-common
    container_name: airflow_init
    user: "root"
    volumes:
      - ./scripts:/scripts
    command: bash -c "sleep 10 && airflow db migrate && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com"
    depends_on:
      postgres:
        condition: service_healthy
    networks: [default]

  airflow-webserver:
    <<: *airflow-common
    container_name: airflow_webserver
    command: airflow webserver
    ports:
      - "8082:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    networks: [default]

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow_scheduler
    command: airflow scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    networks: [default]

  airflow-worker:
    <<: *airflow-common
    container_name: airflow_worker
    command: airflow celery worker
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./hadoop-base/hadoop-conf:/opt/hadoop/etc/hadoop
      - scripts_data:/opt/airflow/scripts
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      airflow-scheduler:
        condition: service_started
      redis:
        condition: service_started
      postgres:
        condition: service_healthy
    networks: [default]

#-----------------------------------------------------------------
volumes:
  namenode_data:
  datanode1_data:
  datanode2_data:
  pg_data:
  scripts_data:
    name: scripts_data
  spark_data:

networks:
  default:
    name: arlequin_default
    driver: bridge
