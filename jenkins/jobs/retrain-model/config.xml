<?xml version='1.1' encoding='UTF-8'?>
<flow-definition plugin="workflow-job@1540.v295eccc9778f">
  <actions>
    <org.jenkinsci.plugins.pipeline.modeldefinition.actions.DeclarativeJobAction plugin="pipeline-model-definition@2.2265.v140e610fe9d5"/>
    <org.jenkinsci.plugins.pipeline.modeldefinition.actions.DeclarativeJobPropertyTrackerAction plugin="pipeline-model-definition@2.2265.v140e610fe9d5">
      <jobProperties/>
      <triggers/>
      <parameters/>
      <options/>
    </org.jenkinsci.plugins.pipeline.modeldefinition.actions.DeclarativeJobPropertyTrackerAction>
  </actions>
  <description>Retain model pipeline</description>
  <keepDependencies>false</keepDependencies>
  <properties/>
  <definition class="org.jenkinsci.plugins.workflow.cps.CpsFlowDefinition" plugin="workflow-cps@4183.v94b_6fd39da_c1">
    <script>pipeline {
  agent any
  environment {
    SPARK_MASTER_URL = &apos;spark://spark-master:7077&apos;
    HDFS_URI         = &apos;hdfs://namenode:9000&apos;
    TRAIN_SCRIPT     = &apos;/scripts/train_model.py&apos;
    MLFLOW_URL       = &apos;http://mlflow:5000&apos;
  }
  stages {
    stage(&apos;Resolve containers&apos;) {
      steps {
        sh &apos;&apos;&apos;
          set -eu
          CID_PY=$(docker ps -qf &quot;label=com.docker.compose.service=pyspark-client&quot; || true)
          if [ -z &quot;$CID_PY&quot; ]; then CID_PY=$(docker ps -qf &quot;name=pyspark-client&quot;); fi
          [ -n &quot;$CID_PY&quot; ] || { echo &quot;[ERROR] pyspark-client no encontrado&quot;; docker ps; exit 1; }
          echo &quot;$CID_PY&quot; &gt; .cid_py
        &apos;&apos;&apos;
      }
    }
    stage(&apos;Train with Spark + log&apos;) {
      steps {
        sh &apos;&apos;&apos;
          set -eu
          CID_PY=$(cat .cid_py)
          METRICS_DIR=&quot;$WORKSPACE/metrics&quot;
          mkdir -p &quot;$METRICS_DIR&quot;
          RUNS_CSV=&quot;$METRICS_DIR/jenkins_runs.csv&quot;
          if [ ! -f &quot;$RUNS_CSV&quot; ]; then echo &quot;timestamp,build,stage,cpu_perc,mem_perc,mem_used_mb,mem_limit_mb,duration_s&quot; &gt; &quot;$RUNS_CSV&quot;; fi

          # Pre-stats
          start_ts=$(date +%s)
          stats=$(docker stats --no-stream --format &apos;{{.CPUPerc}},{{.MemUsage}},{{.MemPerc}}&apos; &quot;$CID_PY&quot; || echo &quot;,,&quot;)
          cpu=$(echo &quot;$stats&quot; | cut -d, -f1 | tr -d &apos;%&apos;)
          mem_perc=$(echo &quot;$stats&quot; | cut -d, -f3 | tr -d &apos;%&apos;)
          mem_pair=$(echo &quot;$stats&quot; | cut -d, -f2 | tr -d &apos; &apos; )
          used=$(echo &quot;$mem_pair&quot; | cut -d/ -f1 | sed &apos;s/[^0-9.]*//g&apos;)
          limit=$(echo &quot;$mem_pair&quot; | cut -d/ -f2 | sed &apos;s/[^0-9.]*//g&apos;)
          echo &quot;$(date -Iseconds),$BUILD_NUMBER,pre,$cpu,$mem_perc,$used,$limit,0&quot; &gt;&gt; &quot;$RUNS_CSV&quot;

          # Training
          docker exec -i &quot;$CID_PY&quot; sh -lc &quot;\
            export MLFLOW_TRACKING_URI=${MLFLOW_URL}; \
            export EXPERIMENT_LOG_FILE=/tmp/arlequin-logs/training_log.csv; \
            mkdir -p /tmp/arlequin-logs; \
            spark-submit --master ${SPARK_MASTER_URL} \
              --conf spark.hadoop.fs.defaultFS=${HDFS_URI} \
              ${TRAIN_SCRIPT}&quot;

          # Post-stats
          end_ts=$(date +%s)
          dur=$((end_ts - start_ts))
          stats=$(docker stats --no-stream --format &apos;{{.CPUPerc}},{{.MemUsage}},{{.MemPerc}}&apos; &quot;$CID_PY&quot; || echo &quot;,,&quot;)
          cpu=$(echo &quot;$stats&quot; | cut -d, -f1 | tr -d &apos;%&apos;)
          mem_perc=$(echo &quot;$stats&quot; | cut -d, -f3 | tr -d &apos;%&apos;)
          mem_pair=$(echo &quot;$stats&quot; | cut -d, -f2 | tr -d &apos; &apos; )
          used=$(echo &quot;$mem_pair&quot; | cut -d/ -f1 | tr -cd &apos;0-9.\n&apos;)
          limit=$(echo &quot;$mem_pair&quot; | cut -d/ -f2 | tr -cd &apos;0-9.\n&apos;)
          echo &quot;$(date -Iseconds),$BUILD_NUMBER,post,$cpu,$mem_perc,$used,$limit,$dur&quot; &gt;&gt; &quot;$RUNS_CSV&quot;

          # Pull training CSV from container (optional)
          docker cp &quot;$CID_PY&quot;:/tmp/arlequin-logs/training_log.csv &quot;$METRICS_DIR/&quot; 2&gt;/dev/null || true
          docker cp &quot;$CID_PY&quot;:/tmp/arlequin-logs/drift_log.csv &quot;$METRICS_DIR/&quot; 2&gt;/dev/null || true
        &apos;&apos;&apos;
      }
    }
    stage(&apos;Evaluate stats&apos;) {
      steps {
        sh &apos;&apos;&apos;
          set -eu
          CID_PY=$(cat .cid_py)
          METRICS_DIR=&quot;$WORKSPACE/metrics&quot;
          mkdir -p &quot;$METRICS_DIR&quot;

          # Ensure SciPy and dependencies for evaluator
          docker exec -i &quot;$CID_PY&quot; sh -lc &apos;python3 - &lt;&lt;PY\ntry:\n import scipy, pandas, numpy\n print(&quot;eval deps OK&quot;)\nexcept Exception:\n raise SystemExit(1)\nPY&apos; || docker exec -i &quot;$CID_PY&quot; sh -lc &apos;python3 -m pip install --no-cache-dir -r /scripts/requirements-drift.txt&apos;

          # Run evaluator inside the container using internal CSVs
          docker exec -i &quot;$CID_PY&quot; sh -lc &quot;python3 /scripts/eval_stats.py --input /tmp/arlequin-logs/training_log.csv --drift-input /tmp/arlequin-logs/drift_log.csv --out /tmp/arlequin-logs/mannwhitney_results.csv --fdr || true&quot;

          # Copy results to workspace
          docker cp &quot;$CID_PY&quot;:/tmp/arlequin-logs/mannwhitney_results.csv &quot;$METRICS_DIR/&quot; 2&gt;/dev/null || true
          docker cp &quot;$CID_PY&quot;:/tmp/arlequin-logs/training_log.csv &quot;$METRICS_DIR/&quot; 2&gt;/dev/null || true
          docker cp &quot;$CID_PY&quot;:/tmp/arlequin-logs/drift_log.csv &quot;$METRICS_DIR/&quot; 2&gt;/dev/null || true
        &apos;&apos;&apos;
      }
    }
  }
  post {
    always {
      echo &apos;Done.&apos;
    }
  }
}
</script>
    <sandbox>true</sandbox>
  </definition>
  <triggers/>
  <authToken>123456</authToken>
  <disabled>false</disabled>
</flow-definition>