# version: "3.9"

x-hadoop-env: &hadoop-common
  CORE_CONF_fs_defaultFS: "hdfs://namenode:9000"
  HDFS_CONF_dfs_replication: "3"

networks:
  backend:

volumes:
  nn-data:
  dn1-data:
  dn2-data:
  dn3-data:

services:
  ################
  #  HDFS LAYER  #
  ################

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    networks:
      - backend
    ports:
      - "9870:9870"   # NameNode UI
      - "9000:9000"   # HDFS RPC
    environment:
      - CLUSTER_NAME=mlops-demo
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - CORE_CONF_dfs_replication=3
      - CORE_CONF_dfs_namenode_http_address=0.0.0.0:9870
    volumes:
      - nn-data:/hadoop/dfs/name

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    networks:
      - backend
    depends_on:
      - namenode
    environment:
      - CLUSTER_NAME=mlops-demo
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - CORE_CONF_dfs_replication=3
    volumes:
      - dn1-data:/hadoop/dfs/data

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    networks:
      - backend
    depends_on:
      - namenode
    environment:
      - CLUSTER_NAME=mlops-demo
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - CORE_CONF_dfs_replication=3
    volumes:
      - dn2-data:/hadoop/dfs/data

  datanode3:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode3
    networks:
      - backend
    depends_on:
      - namenode
    environment:
      - CLUSTER_NAME=mlops-demo
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - CORE_CONF_dfs_replication=3
    volumes:
      - dn3-data:/hadoop/dfs/data

  ################
  #  SPARK LAYER #
  ################

  spark-master:
    image: bitnami/spark:3.5.6
    container_name: spark-master
    networks: [backend]
    ports:
      - "8080:8080"   # Spark Master UI
      - "7077:7077"   # Spark RPC
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      # --- arreglos Ivy ---
      - HOME=/tmp                     # ruta siempre escribible
      - SPARK_JARS_IVY=/tmp/.ivy2     # cache Ivy
    depends_on: [namenode, kafka]

  spark-worker-1:
    image: bitnami/spark:3.5.6
    container_name: spark-worker-1
    networks: [backend]
    ports:
      - "8081:8081"   # Worker 1 UI
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - HOME=/tmp
      - SPARK_JARS_IVY=/tmp/.ivy2
    depends_on: [spark-master]

  spark-worker-2:
    image: bitnami/spark:3.5.6
    container_name: spark-worker-2
    networks: [backend]
    ports:
      - "8082:8081"   # Worker 2 UI (mapped al 8081 interno)
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - HOME=/tmp
      - SPARK_JARS_IVY=/tmp/.ivy2
    depends_on: [spark-master]

  spark-worker-3:
    image: bitnami/spark:3.5.6
    container_name: spark-worker-3
    networks: [backend]
    ports:
      - "8083:8081"   # Worker 3 UI (mapped al 8081 interno)
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - HOME=/tmp
      - SPARK_JARS_IVY=/tmp/.ivy2
    depends_on: [spark-master]

  spark-client:
    image: bitnami/spark:3.5.6
    container_name: spark-client
    networks: [backend]
    depends_on: [spark-master]
    environment:
      - SPARK_MODE=client                   # ← lanza un nodo ad-hoc
      - SPARK_MASTER_URL=spark://spark-master:7077
      # Caché Ivy simple y escribible
      - HOME=/tmp
      - SPARK_JARS_IVY=/tmp/.ivy2
    volumes:
      - ./scripts:/workspace
    # Arranca PySpark con los paquetes requeridos
    command:
      - "pyspark"
      - "--master"
      - "spark://spark-master:7077"
      - "--packages"
      - "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.6,org.apache.spark:spark-avro_2.12:3.5.6"

  ##################
  #  KAFKA LAYER   #
  ##################
  zookeeper:
    image: bitnami/zookeeper:3.9
    container_name: zookeeper
    networks: [backend]
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    ports:
      - "2181:2181"

  kafka:
    image: bitnami/kafka:3.7
    container_name: kafka
    networks: [backend]
    depends_on: [zookeeper]
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_LISTENERS=INTERNAL://kafka:9093,EXTERNAL://0.0.0.0:9092
      - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka:9093,EXTERNAL://localhost:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_INTER_BROKER_LISTENER_NAME=INTERNAL
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CREATE_TOPICS=inference_raw:3:3,metrics:3:3,alerts:3:3
      # === Límites ampliados ===
      - KAFKA_CFG_MESSAGE_MAX_BYTES=10000000        # 10 MB
      - KAFKA_CFG_REPLICA_FETCH_MAX_BYTES=15000000  # /+
      - KAFKA_CFG_FETCH_MAX_BYTES=15000000
      - KAFKA_CFG_SOCKET_REQUEST_MAX_BYTES=15000000